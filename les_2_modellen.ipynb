{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les 2: Modellen\n",
    "\n",
    "Nadat we in de vorige les hebben geleerd hoe je met behulp van statistiek analyses kunt doen van je data, gaan we in deze les de computer aan het werk zetten om de geleerde analysetechnieken in te zetten om voorspellingen te doen.\n",
    "\n",
    "Dat is immers waar het bij Data Science allemaal om draait: de computer leren patronen te herkennen in onze data zodat het nemen van beslissingen zoveel mogelijk kan worden geautomatiseerd.\n",
    "\n",
    "De volgende onderwerpen komen in dit notebook aan bod:\n",
    "\n",
    "- Correlaties\n",
    "- Lineaire regressie-analyse\n",
    "- Clustering\n",
    "- Decision trees\n",
    "- Ensemble models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitvoeren op Google Colab\n",
    "\n",
    "Dit notebook kan worden uitgevoerd op Google Colab. Hiervoor is een Google-account vereist.\n",
    "\n",
    "Klik op de knop \"Open in Google Colab\" om het notebook te openen in Google Colab:\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/mcdejonge/cursus-beginnen-met-data-science/blob/main/les_2_modellen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In het codeblok hieronder worden alle bibliotheken ingeladen die in dit notebook worden gebruikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Correlaties en lineaire verbanden\n",
    "\n",
    "Voordat we de computer echt aan het werk kunnen zetten, moeten we ons repertoire aan statistische kennis nog iets verder uitbreiden. We moeten weten hoe je vast kunt stellen dat er een verband bestaat tussen twee variabelen.\n",
    "\n",
    "Stel, we meten elke dag wat de gemiddelde temperatuur per dag is. Dat is onze eerste variabele. Daarnaast houden we per dag bij hoeveel ijsjes er zijn verkocht - onze tweede variabele. Waar we uiteindelijk naartoe willen, is een model dat geheel automatisch voor ons op basis van de weersverwachting voor de komende dagen kan voorspellen hoeveel ijsjes er verkocht gaan worden. Een dergelijk model is bijvoorbeeld handig voor uitbaters van strandtenten, ijsfabrikanten enzovoort.\n",
    "\n",
    "Laten we om te beginnen eens kijken naar onze data. Omdat er twee kolommen (variabelen) zijn, kunnen we een scatterplot maken om het verband tussen de variabelen in beeld te brengen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een nieuw dataframe met fictieve gegevens over het aantal\n",
    "# verkochte ijsjes (per 100k) en de gemiddelde temperatuur (in graden Celcius)\n",
    "dfijsjes = pd.DataFrame({\n",
    "    'temperatuur' : [3.3, 8.2,  9.8,  11.3, 12.8, 15.1, 17.5],\n",
    "    'ijsjes' :      [8.8, 11.2, 11.3, 15.2, 16.1, 21.7, 22.4]\n",
    "})\n",
    "\n",
    "# We maken een scatterplot van ons dataframe om het verband tussen de\n",
    "# twee variabelen in beeld te brengen.\n",
    "plt.scatter(dfijsjes['temperatuur'], dfijsjes['ijsjes'])\n",
    "# Vanaf nu gaan we onze grafiekjes netjes labelen zodat onmiddellijk duidelijk is waar we naar kijken.\n",
    "plt.title('Verkochte ijsjes vs temperatuur')\n",
    "plt.ylabel('Verkochte ijsjes (100k stuks)')\n",
    "plt.xlabel('Gemiddelde dagtemperatuur (C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er lijkt een duidelijk verband te zitten tussen de gemiddelde dagtemperatuur en het aantal verkochte ijsjes. Verder lijkt het erop alsof dat verband *lineair* is - dat wil zeggen dat je een rechte lijn zou kunnen trekken door alle datapunten heen waarbij de afstand tussen die lijn en de datapunten klein blijft.\n",
    "\n",
    "Het trekken van zo'n lijn is een werkje dat je beter door een computer kunt laten doen (er komt erg veel - op zichzelf eenvoudig - rekenwerk bij kijken). Hoe je dit precies doet, komt aan bod in paragraaf 2.2, \"Ons eerste model: lineaire regressie\". Voor nu is het voldoende als we op het oog herkennen dat je een rechte lijn zou *kunnen* trekken door deze datapunten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat we ook kunnen doen, is de computer laten uitrekenen hoe *sterk* het lineaire verband is tussen de twee variabelen.\n",
    "\n",
    "Een andere term voor \"verband tussen twee variabelen\" is *correlatie*, vandaar dat we meestal zeggen dat we de *correlatie* gaan berekenenen tussen twee variabelen.\n",
    "\n",
    "In Python gaat dat heel eenvoudig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfijsjes['ijsjes'].corr(dfijsjes['temperatuur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De berekende correlatie is `0,95`. Dat is enorm hoog - doorgaans zijn we al tevreden met een correlatie van `0.7`. In de sociale wetenschappen en ook in bedrijfskundig onderzoek zijn zelfs nóg lagere correlaties gebruikelijk.\n",
    "\n",
    "De oorzaak voor deze hoge correlatie is natuurlijk dat de gebruikte data kunstmatig is en speciaal gemaakt is om uit te leggen wat correlaties en lineaire verbanden zijn.\n",
    "\n",
    "Hieronder zien we een voorbeeld van twee variabelen waar juist helemaal geen verband tussen bestaat (beide reeksen cijfers zijn volkomen willekeurig gegenereerd). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoe deze code werkt, is voor nu niet zo interessant. Het gaat om de grafiek.\n",
    "np.random.seed(39)\n",
    "dfrandom = pd.DataFrame({\n",
    "    # Maak 2 reeksen van willekeurig gegenereerde maar normaal verdeelde getallen.\n",
    "    'var1' : np.random.normal(4, 2, 10),\n",
    "    'var2' : np.random.normal(6, 8, 10)\n",
    "})\n",
    "plt.scatter(dfrandom['var1'], dfrandom['var2'])\n",
    "plt.title(f\"Twee willekeurige getallenreeksen. De correlatie is {dfrandom['var1'].corr(dfrandom['var2']):.2f}\")\n",
    "plt.xlabel('Variabele 1')\n",
    "plt.ylabel('Variabele 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien datapunten die willekeurig over de grafiek zijn verdeeld (wat logisch is, aangezien ze willekeurig zijn gegenereerd). De correlatie is dan ook laag: `0,12`, wat zoveel betekent als: *geen correlatie van betekenis*.\n",
    "\n",
    "Het is op zich mogelijk om een lijn te trekken door deze datapunten, maar elk datapunt zou dan erg ver van de lijn liggen. De betekenis van de lijn (het lineaire verband) is dan niet groot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Wat is de betekenis van een hoge correlatie?\n",
    "\n",
    "Wat een hoge correlatie precies betekent, hangt af van het soort data waarvoor je de correlatie berekent.\n",
    "\n",
    "In het voorbeeld van de temperatuur en het aantal verkochte ijsjes betekent een hoge correlatie: \"als de gemiddelde dagtemperatuur omhoog gaat, stijgt het aantal verkochte ijsjes\".\n",
    "\n",
    "Rekenkundig gezien geeft de waarde voor de correlatie, de zogenoemde *correlatie-coëfficiënt* weer *welk deel* van de verandering in de ene variabele (`b`) verklaard wordt door een verandering in de andere variabele (`a`). Is de correlatie-coëfficiënt 1, dan veroorzaakt elke verandering in variabele `a` een precies even grote verandering in variabele `b`. Is de correlatie-coëfficiënt 0,5, dan verklaart elke verandering in variabele `a` de *helft* van de verandering in variabele `b` en is de correlatie-coëfficiënt 0 dan hebben veranderingen in `a` en `b` helemaal niets met elkaar te maken.\n",
    "\n",
    "> **Belangrijk** hoewel we zeggen dat veranderingen in `a` veranderingen in `b` \"verklaren\", betekent dat *niet* dat correlatie een oorzaak-gevolg relatie aanduidt. Het is nadrukkelijk *niet* zo dat je kunt stellen dat een hogere temperatuur een grotere ijsverkoop *tot gevolg* heeft - je kunt alleen zeggen dat een hogere temperatuur *samenhangt* met een grotere ijsverkoop.\n",
    ">\n",
    "> Een voorbeeld om dit te verduidelijken is bijvoorbeeld het volgende: stel dat we alleen gegevens hebben over het aantal verkochte ijsjes én over het aantal keren dat de reddingsbrigade uit moet varen op een dag. Ook tussen deze twee variabelen is de correlatie sterk en waarschijnlijk vrijwel net zo sterk als tussen het aantal verkochte ijsjes en de gemiddelde temperatuur. Toch zal het onmiddellijk duidelijk zijn dat er niet meer ijsjes worden verkocht alleen omdat er ergens in Scheveningen een boot met reddingswerkers de zee op gaat.\n",
    ">\n",
    "> We zeggen daarom ook wel: **correlatie is geen causaliteit** (causaliteit: een *oorzakelijk* verband)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "In les 1 hebben we door onze oogharen gekeken naar het verband tussen snavellengte (`culmen_length_mm`) en zwemvlieslengte (`flipper_length_mm`). We dachten een lineair verband te kunnen zien.\n",
    "\n",
    "Nu gaan we berekenen hoe sterk dat verband is. Lees de pinguin-data in (`data/palmerpenguins/penguins_size_clean.csv`) en laat Python de correlatie-coëfficiënt berekenen voor de kolommen `culmen_length_mm` en `flipper_length_mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlezen van penguin-data. \n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    dfpenguins = pd.read_csv(\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/data/palmerpenguins/penguins_size_clean.csv\")\n",
    "else:\n",
    "    dfpenguins = pd.read_csv(\"data/palmerpenguins/penguins_size_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plaats hier je voorbeeldcode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Het eerste model: lineaire regressie\n",
    "\n",
    "Nu we hebben vastgesteld dat er een sterke correlatie is tussen de gemiddelde temperatuur en de hoeveelheid verkochte ijsjes, kunnen we de computer inzetten om ons te helpen voorspellen hoeveel ijsjes er morgen verkocht gaan worden gegeven de verwachte gemiddelde dagtemperatuur (die je natuurlijk bij het KNMI kunt opvragen).\n",
    "\n",
    "We weten ook dat het verband lineair is: hoe hoger de gemiddelde temperatuur, hoe meer ijsjes er verkocht worden.\n",
    "\n",
    "Wat we de computer zouden willen vragen, is om voor ons een *model* te maken van dit lineaire verband. Zo'n model is een stuk software waarin zo'n verband is vastgelegd en waarmee je voorspellingen kunt doen, in ons geval door er een verwachte temperatuur in te stoppen waarna er een verwacht aantal verkochte ijsjes uitrolt.\n",
    "\n",
    "De Python-bibliotheek `scikit-learn` bevat allemaal gereedschap om dit soort modellen te maken. Hieronder gebruiken we de functie \"LinearRegression\" om een lineair model te maken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een nieuw, leeg lineair model aan\n",
    "model = LinearRegression() \n",
    "\n",
    "# Geef het model onze data zodat het kan uitrekenen wat precies \n",
    "# het verband is tussen de temperatuur en het aantal verkochte ijsjes.\n",
    "model.fit(dfijsjes[['temperatuur']], dfijsjes[['ijsjes']])\n",
    "\n",
    "# Nu het model weet wat het verband is, kunnen we voorspellingen doen.\n",
    "# Hoeveel ijsjes (x 100k) gaan we verkopen als de gemiddelde temperatuur\n",
    "# 30 graden is?.\n",
    "# Merk op dat modellen alleen met dataframes overwegkunnen. \n",
    "# We moeten onze temperatuur dus in een dataframe stoppen.\n",
    "model.predict(pd.DataFrame({'temperatuur' : [30]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helaas kan `LinearRegression` niet zonder meer met eenvoudige reeksen getallen overweg. Dit verplicht ons om een paar extra bewerkingen te doen voordat we het model onze data geven. Dat is waarom we het model niet `dfijsjes['temperatuur]` geven maar `dfijsjes[['temperatuur']]` (hetzelfde geldt voor `dfijsjes['ijsjes']`).\n",
    "\n",
    "Om dezelfde reden kunnen we het model niet het aantal verkochte ijsjes laten voor spellen voor één enkele temperatuur maar moeten we die ene temperatuur in een dataframe stoppen. Het resultaat is eveneens niet een enkel getal of zelfs een lijst met getallen maar een lijst met lijsten van getallen oftewel in dit geval een getal tussen *twee* sets blokhaken (`[[30]]`).\n",
    "\n",
    "In de praktijk is dit geen probleem, omdat je altijd met grotere hoeveelheden gegevens werkt, maar voor dit eerste voorbeeld is er dus wat extra werk nodig.\n",
    "\n",
    "In ieder geval kunnen we nu zien hoeveel ijsjes er worden verkocht als de gemiddelde dagtemperatuur 30 graden is. Voor ijsjesfabrikanten en uitbaters van strandtenten is dit natuurlijk buitengewoon nuttig om te weten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Hoe goed presteert een lineair model?\n",
    "\n",
    "Voordat een ijsjesfabrikant op basis van een lineair model nieuwe machines gaat bestellen, is het van belang  te weten hoe *goed* de voorspellingen van dat model eigenlijk zijn.\n",
    "\n",
    "Hiervoor heeft een `LinearModel` de functie `score` aan boord. Deze functie berekent een numerieke waarde die weergeeft hoe ver de voorspellingen van het model afwijken van de werkelijkheid.\n",
    "\n",
    "Hieronder zien we een voorbeeld.\n",
    "\n",
    "Stel dat we gegevens hebben over de verkoop van ijsjes voor gemiddelde dagtemperatuur die het lineaire model nog niet eerder heeft gezien. Waar we benieuwd naar zijn is hoe ver de voorspellingen van het model voor de dagtemperaturen in kwestie afwijken van de daadwerkelijke aantallen verkochte ijsjes. \n",
    "\n",
    "Hier zijn de waarden die we willen testen:\n",
    "\n",
    "| Temperatuur | Aantal ijsjes x 100k |\n",
    "|-------------|----------------------|\n",
    "| 14          | 15,8                 |\n",
    "| 18          | 23                   |\n",
    "| 20          | 24                   |\n",
    "\n",
    "We gaan met model vragen om de score te berekenen op basis van deze waarden. Hoeveer zat het model er bij het voorspellen naast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak opnieuw een nieuw, leeg lineair model aan en vul het met data.\n",
    "model = LinearRegression() \n",
    "model.fit(dfijsjes[['temperatuur']], dfijsjes[['ijsjes']])\n",
    "\n",
    "# Maak een dataframe met de waarden die we willen testen.\n",
    "testwaarden = pd.DataFrame({\n",
    "    'temperatuur' : [14, 18, 20],\n",
    "    'ijsjes' : [15.8, 23,24]\n",
    "})\n",
    "# Bereken de afwijking tussen de voorspelde waarden en de echte.\n",
    "model.score(testwaarden[['temperatuur']], testwaarden[['ijsjes']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De methode `score` berekent de zogenoemde $R^2$ waarde (R Squared). Dit is in dit geval dezelfde meetwaarde als de correlatie-coëfficiënt die we eerder zagen.\n",
    "\n",
    "De berekende score is lager dan de correlatie-coëfficient maar dat is logisch omdat de score is gebaseerd op data die het model nog niet eerder gezien heeft.\n",
    "\n",
    "De score is echter wel heel hoog. Net als bij de correlatie-coëfficient geldt een waarde groter dan `0,7` als \"zeer goed\".\n",
    "\n",
    "Overigens is het mogelijk om een lineair model waar één variabele wordt voorspeld op basis van één andere variabele in beeld te brengen. De techniek daarvoor valt buiten de stof van deze les, dus de precieze werking van deze  code is voor nu verder niet belangrijk, maar voor dit model ziet de lijn er zo uit:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(dfijsjes['temperatuur'], dfijsjes['ijsjes'], alpha=0.5)\n",
    "plt.plot(dfijsjes['temperatuur'], model.predict(dfijsjes[['temperatuur']]), color='darkred')\n",
    "ax.set_title(\"Lineair model voor temperatuur vs ijsjes\")\n",
    "plt.ylabel('Verkochte ijsjes (100k stuks)')\n",
    "plt.xlabel('Gemiddelde dagtemperatuur (C)')\n",
    "ax.legend(['Echte waarden'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Wat als het verband niet lineair is?\n",
    "\n",
    "Veel verbanden in de echte wereld zijn lineair - of in ieder geval lineair genoeg om voorspellingen te kunnen doen met een lineair model. Voor veel andere verbanden geldt dat echter niet.\n",
    "\n",
    "Hier is bijvoorbeeld opnieuw een scatterplot voor het aantal verkochte ijsjes vs de gemiddelde dagtemperatuur, maar nu uitgebreid met wat extra observaties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een nieuw dataframe met extra gegevens over het aantal\n",
    "# verkochte ijsjes (per 100k) en de gemiddelde temperatuur (in graden Celcius)\n",
    "dfijsjes = pd.DataFrame({\n",
    "    'temperatuur' : [3.3, 4.4, 5.3, 6.2, 7.4, 8.2,  9.8,  10.4, 11.3, 12.8, 13.3, 14.5, 15.1, 16.7, 17.5, 18.1, 19.8, 21.0, 22.2, 23.8 ],\n",
    "    'ijsjes' :      [8.8, 7.8, 8.1, 8.3, 9.1, 11.2, 11.3, 11.4, 15.2, 16.1, 17.6, 19.2, 21.7, 22.2, 22.4, 23.6, 23.5, 23.6, 22.4, 21.6]\n",
    "})\n",
    "\n",
    "# We maken een scatterplot van ons dataframe om het verband tussen de\n",
    "# twee variabelen in beeld te brengen.\n",
    "plt.scatter(dfijsjes['temperatuur'], dfijsjes['ijsjes'])\n",
    "# Vanaf nu gaan we onze grafiekjes ook netjes labelen zodat onmiddellijk duidelijk is waar we naar kijken.\n",
    "plt.title('Verkochte ijsjes vs temperatuur')\n",
    "plt.ylabel('Verkochte ijsjes (100k stuks)')\n",
    "plt.xlabel('Gemiddelde dagtemperatuur (C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat het aantal verkochte ijsjes redelijk gelijkblijft tot 7,5 graden, dan sterk stijgt en vanaf 17,5 graden afvlakt (en zelfs iets daalt). Wanneer we voor deze dataset een lineair model zouden gebruiken, dan zou het voorspelde aantal verkochte ijsjes op hele warme dagen (een daggemiddelde van meer dan 22 graden) waarschijnlijk veel te hoog zijn (en wellicht voor dagen met een laag daggemiddelde juist te laag).\n",
    "\n",
    "Het is dan ook van belang om goed te begrijpen hoe lineair de data is en wat de gevolgen zijn van een verkeerde voorspelling voordat je aan de slag gaat met een lineair model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "1. Gebruik de pinguindata uit de vorige opgave. Maak een lineair model dat op basis van de zwemvlieslengte  (`flipper_length_mm`) de snavellengte (`culmen_length_mm`) voorspelt.\n",
    "2. Bereken de score van het model. Hiervoor kun je de dataset zelf gebruiken. Dit vertelt je hoe sterk de voorspellingen van het model afwijken van de echte waarden.\n",
    "3. Wat valt op? Is het resultaat wat je had verwacht?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plaats hier je eigen code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Het tweede model: clustering\n",
    "\n",
    "Lineaire modellen zijn nuttig voor gevallen waarin je wil voorspellen hoe *groot* een bepaalde wordt als je een andere waarde aanpast (bijvoorbeeld *hoeveel* ijsjes je gaat verkopen als de gemiddelde dagtemperatuur wijzigt) oftewel waarin de te voorspellen waarde een *getal* is.\n",
    "\n",
    "Er zijn echter ook gevallen waarin de waarde die je wil voorspellen een zg. *categorie* is. Stel je een supermarkt voor die gegevens heeft over het aantal pakken hagelslag en het aantal gekochte diepvriespizza's dat er aan iedere klant is verkocht. De supermarkt zou graag willen weten of er misschien verschillende *groepen* of *categorieën* te onderscheiden zijn in het klantenbestand (bijvoorbeeld om gericht te kunnen adverteren voor een nieuw soort vruchtenhagel).\n",
    "\n",
    "Om hier achter te komen, moet de supermarkt een zogenoemde *clusteranalyse* uitvoeren. Een dergelijke analyse houdt in dat de computer *clusters* oftewel *categorieën* gaat ontdekken in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Clustering met K-means\n",
    "\n",
    "Er zijn allemaal verschillende algoritmen om clusteranalyse uit te voeren. In deze les beperken we ons tot het algoritme `K-means`. In dit algoritme probeert de computer groepen (clusters) te vinden door voor elk datapunt te kijken of de waarde van een bepaalde variabele zich in de buurt bevindt van de gemiddelde (\"mean\") waarde voor die variabele.\n",
    "\n",
    "Wat dit precies inhoudt, is het gemakkelijkst te zien aan de hand van een eenvoudig voorbeeld. \n",
    "\n",
    "We gaan kijken of een computer, gegeven een verzameling (fictieve) verkoopgegevens van een supermarkt kan ontdekken dat er twee soorten klanten zijn: klanten die veel hagelslag kopen en weinig diepvriespizza's en klanten bij wie dat andersom is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlezen van supermarktdata. \n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    dfsuper = pd.read_csv(\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/data/supermarkt.csv\")\n",
    "else:\n",
    "    # Merk op dat we alleen de drie kolommen inladen die we daadwerkelijk willen gebruiken in deze les.\n",
    "    dfsuper = pd.read_csv(\"data/supermarkt.csv\")[['klant', 'diepvriespizza', 'hagelslag']]\n",
    "dfsuper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat het maar om 10 datapunten gaat, kunnen we zelf ook wel zien dat er een duidelijk verschil is tussen klanten die veel diepriespizza's kopen en weinig hageslag en andersom. Dat biedt ons de gelegenheid om te controleren of de computer dat verschil net zo goed kan opmerken als wijzelf.\n",
    "\n",
    "We gaan de functie `KMeans` gebruiken om de klantcategorieën op te sporen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We maken een nieuw KMeans-model aan. We willen dat onze data wordt opgedeeld in 2 clusters.\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "# Train het model op de twee numerieke kolommen van de supermarktdata.\n",
    "kmeans.fit(dfsuper[['diepvriespizza', 'hagelslag']])\n",
    "\n",
    "# Voor het overzicht gebruiken we de functie `assign` om de voorspelde clusters te tonen naast de oorspronkelijke data\n",
    "dfsuper.assign(cluster = kmeans.predict(dfsuper[['diepvriespizza', 'hagelslag']]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De resultaten zijn verbluffend - wat natuurlijk komt omdat het om erg weinig data ging die speciaal was gemaakt om gemakkelijk in clusters ingedeeld te worden.\n",
    "\n",
    "Nu we weten dát het werkt, kunnen we kijken hóe.\n",
    "\n",
    "Hiervoor moeten we een scatterplot maken waarin we het aantal gekochte diepvriespizza's afzetten tegen het aantal gekochte pakken hagelslag. Dat hebben we eerder gedaan in les 1. Wat we nu echter gaan toevoegen is een parameter `c` (color) met daarin de variabele die we willen gebruiken om de punten verschillende kleuren te geven.\n",
    "\n",
    "Op de x-as plaatsen we het aantal verkochte diepvriespizza's, op de y-as het aantal verkochte pakken hagelslag en de kleur geeft aan in welk cluster de klant is ingedeeld door het K-means algoritme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een scatterplot. Gebruik de parameter 'c' (color) om de kolom 'cluster' (dwz de voorspelde klantcategorie) de kleur van de punten te laten bepalen.\n",
    "plt.scatter(\n",
    "    dfsuper['diepvriespizza'], \n",
    "    dfsuper['hagelslag'], \n",
    "    c=kmeans.predict(dfsuper[['diepvriespizza', 'hagelslag']]))\n",
    "plt.title('Aantal verkochte diepvriespizza\\'s en pakken hagelslag per klant')\n",
    "plt.xlabel('Aantal diepvriespizza\\'s')\n",
    "plt.ylabel('Aantal pakken hagelslag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat we zien, is dat als je de gegevens voor elke klant in een grafiek zet waarbij de positie van elke klant wordt bepaald door de waarde van twee variabelen, er duidelijk te onderscheiden groepen ontstaan, simpelweg door te kijken naar de waarde van die twee variabelen.\n",
    "\n",
    "Dit werkt ook met drie, vijf of duizend variabelen. Je kunt het dan echter niet meer in een grafiek weergeven, vandaar dat we ons hier beperken tot twee. In de praktijk kun je echter zoveel variabelen gebruiken als je wil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 K-means clustering met echte data\n",
    "\n",
    "De data die we zojuist hebben gebruikt, was speciaal gemaakt voor dit voorbeeld en liet zich dus gemakkelijk clusteren. Werk je met echte data, dan zijn de resultaten vaak lang niet zo netjes.\n",
    "\n",
    "Gebruik de penguin-data om op basis van de kolommen culmen_length_mm en flipper_length_mm de soort te voorspellen:\n",
    "\n",
    "1. Maak een nieuw KMeans-model met 3 clusters (er zijn drie soorten).\n",
    "2. Train het (`fit`) op de kolommen 'culmen_length_mm' en 'flipper_length_mm'.\n",
    "3. Plot het resultaat door een scatterplot te maken waarin je de kolommen 'culmen_length_mm' en 'flipper_length_mm' tegen elkaar afzet. Gebruik de clusters die worden voorspeld met `kmeans.predict()` voor de kleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlezen van penguin-data. \n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    dfpenguins = pd.read_csv(\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/data/palmerpenguins/penguins_size_clean.csv\")\n",
    "else:\n",
    "    dfpenguins = pd.read_csv(\"data/palmerpenguins/penguins_size_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plaats hier je eigen code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te kunnen beoordelen hoe goed het model het heeft gedaan, is wat gegoochel met matplotlib nodig. Het voert te ver om dat in deze cursus uit te leggen. Voorbeeldcode hiervoor vind je echter in de map \"docent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Decision trees\n",
    "\n",
    "Een andere manier om data te classificeren is gebruikmaken van zogenoemde *Decision Trees* (*beslisbomen*). Een Decision Tree bestaat uit een reeks ja / nee-beslissingen die uiteindelijk leiden tot de keuze voor een bepaalde categorie.\n",
    "\n",
    "Bijvoorbeeld: stel dat we willen weten met wat voor soort dier te maken we hebben. We gebruiken hiervoor de volgende drie vragen:\n",
    "- Heeft het dier vleugels?\n",
    "- Kan het dier vliegen?\n",
    "- Heeft het dier klauwen?\n",
    "\n",
    "De decision tree ziet er dan uit als volgt:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/img/decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een dergelijke beslisboom kun je ook met een computer maken. Je hoeft hiervoor niet zelf te bepalen welke vragen uiteindelijk leiden tot een bepaalde keuze. Dit doet de computer voor je, en wel door op de één of andere manier te komen tot een set vragen die aan het einde zo homogeen mogelijke groepen opleveren.\n",
    "\n",
    "In het voorbeeld hierboven beginnen met een groep waarin vier verschillende diersoorten zitten. Die brengen we terug naar twee en vervolgens naar één. We reduceren dus steeds het aantal verschillende diersoorten in de groep die overblijft oftewel we maken hem steeds homogener.\n",
    "\n",
    "Hieronder zien we hoe je deze beslisboom met een `DecisionTreeClassiefier` namaakt.\n",
    "\n",
    "Merk op dat de gegenereerde boom iedere keer dat je de code opnieuw uitvoert anders is. Dat heeft te maken met de manier waarop het algoritme werkt. Omdat er in dit voorbeeld zo weinig data is, maakt de volgorde van de vragen eigenlijk niet uit, waardoor je steeds een andere startvraag kunt krijgen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lees de data in.\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    dfdieren = pd.read_csv(\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/data/dieren.csv\")\n",
    "else:\n",
    "    dfdieren = pd.read_csv(\"data/dieren.csv\")\n",
    "dfdieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Deze kolommen gebruiken we om de keuze te maken\n",
    "xcols = ['heeft_vleugels', 'kan_vliegen', 'heeft_klauwen']\n",
    "# Deze kolom willen we voorspellen\n",
    "ycol = 'dier'\n",
    "\n",
    "# Maak een tree met diepte 2\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "tree.fit(dfdieren[xcols], dfdieren[[ycol]])\n",
    "plt.figure(figsize=(25,20))\n",
    "plot_tree(tree,\n",
    "          feature_names = xcols,\n",
    "          class_names = dfdieren[ycol].unique(),\n",
    "          proportion = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk op dat de grafische weergave niet geschikt is om te achterhalen wat er precies wordt voorspeld voor een specifieke invoer (deze weergave is vooral bedoeld om de omvang van de boom in beeld te brengen). Dat de boom echter wel gewoon werkt zoals de bedoeling is, kun je aantonen door iets te laten voorspellen. `[0, 0, 0]` is bijvoorbeeld een dolfijn en `[1,1,0]` een merel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controleer of het model correct voorspelt.\n",
    "tree.predict(np.array([1,1,0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een interessanter voorbeeld is het volgende: gegeven een dataset met verkoopgegevens voor autozitjes, is het mogelijk om te voorspellen of er van een bepaald product meer of minder dan gemiddeld wordt verkocht?\n",
    "\n",
    "De waarde die we willen voorspellen is 'Above_Below_Avg_Loc' (1 als het aantal verkochte exemplaren groter is dan het gemiddelde aantal verkochte exemplaren per product *gegeven de **loc**atie waar het product in de winkel wordt tentoongesteld*, anders 0).\n",
    "\n",
    "De overige kolommen worden gebruikt om de boom mee samen te stellen.\n",
    "\n",
    "Hieronder zien we hoe dit werkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlezen van auto-data. \n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    dfcar = pd.read_csv(\"https://raw.githubusercontent.com/mcdejonge/cursus-beginnen-met-data-science/refs/heads/main/data/carseats/carseats_processed.csv\")\n",
    "else:\n",
    "    dfcar = pd.read_csv(\"data/carseats/carseats_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kolommen die worden gebruikt om de keuze te maken:\n",
    "xcols = ['ShelveLoc_Bad',\n",
    "         'ShelveLoc_Good',\n",
    "         'ShelveLoc_Medium',\n",
    "         'Urban_No',\n",
    "         'Urban_Yes',\n",
    "         'US_No',\n",
    "         'US_Yes',\n",
    "         'Sales',\n",
    "         'CompPrice',\n",
    "         'Income',\n",
    "         'Advertising',\n",
    "         'Population',\n",
    "         'Price','Age',\n",
    "         'Education']\n",
    "# Kolom die voorspeld moet worden\n",
    "ycol = 'Above_Below_Avg_Loc'\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(dfcar[xcols], dfcar[[ycol]])\n",
    "plt.figure(figsize=(25,20))\n",
    "plot_tree(tree,\n",
    "          feature_names = xcols,\n",
    "          class_names = ['BelowAvg', 'AboveAvg'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De resulterende boom is vrij complex, wat misschien logisch is omdat er zoveel variabelen zijn om de beslissing op te baseren, maar wat opvalt is dat veel van de variabelen helemaal niet worden gebruikt. In plaats daarvan komt dezelfde variabele vaker dan eens terug in de boom.\n",
    "\n",
    "Op zich hoeft dat geen probleem te zijn, natuurlijk, maar het geeft wel aan dat een door een computer gegenereerde decision tree soms moeilijk te volgen kan zijn.\n",
    "\n",
    "In situaties waar het belangrijk is om voorspellingen uit te leggen, is een decision tree dan ook lang niet altijd een goede keuze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Ensemble models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sommige gevallen levert één enkele decision tree een onvoldoende goed model op. Zoals we bij het voorbeeld hierboven bij het classificeren van dieren hebben gezien, zijn decision trees erg gevoelig voor de toevallige keuze voor de eerste vraag. Ook zijn ze gevoelig voor \"ruis\" in de data. Om deze problemen te omzeilen, wordt er daarom soms gebruikgemaakt van zogenoemde *ensemble models*. Een *ensemble model* is een model dat bestaat uit meerdere andere modellen (een \"ensemble van modellen\", dus). Het idee is dat elk van de modellen waaruit het ensemble model bestaat, wordt getraind op de data of op een deel van de data, waarna de voorspelling wordt gedaan door een combinatie van deze modellen.\n",
    "\n",
    "Er zijn allemaal verschillende technieken om dit te doen, zoals:\n",
    "- **Bagging** : Verschillende decision trees worden afzonderlijk van elkaar getraind. Deze decision trees doen vervolgens afzonderlijk hun voorspellingen. Deze voorspellingen worden samengevoegd en de vaakst voorkomende voorspelling wordt als uiteindelijke voorspelling gebruikt.\n",
    "- **Boosting** : Net als bij bagging worden er verschillende decision trees getraind, maar nu worden ze niet afzonderlijk getraind maar na elkaar. Iedere opeenvolgende decision tree richt zich speciaal op de gevallen die door de voorgaande trees verkeerd zijn voorspeld (de verkeerde voorspellingen krijgen als het ware een *boost*). Het resultaat zou dan een model moeten zijn dat allerlei verschillende gevallen goed kan voorspellen.\n",
    "\n",
    "Hierboven is van decision trees al gezegd dat het vaak lastig is om uit te leggen wat ze precies doen. Bij ensemble modellen is dit probleem nog groter: van een combinatie van tientallen of zelfs honderden decision trees is met geen mogelijkheid vast te stellen hoe een bepaalde voorspelling tot stand is gekomen. Ondanks hun vaak goede prestaties zijn ze daarom voor veel machine learning-toepassingen dan ook niet geschikt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio-opdracht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ga op zoek naar een dataset, bijvoorbeeld van je eigen werk (of gebruik de dataset die je hebt gebruikt voor de portfolio-opdracht van de vorige les) en probeer één van de variabelen te voorspellen met behulp van de andere variabelen. Gebruik één van de modellen die hierboven zijn behandeld: lineaire regressie, k-means clustering of een decision tree. Hoe goed werkt je model (of juist niet)? Hoe komt dat?\n",
    "\n",
    "Lever je uitwerking van de opdracht in als notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
